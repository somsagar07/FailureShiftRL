{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import gym\n",
    "import cv2\n",
    "import math\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16)\n",
    "pipe.load_lora_weights(\"lora weights/unique_artist.safetensors\", weight_name=\"pytorch_lora_weights.safetensors\")\n",
    "pipe = pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(random):\n",
    "    prompts = [\n",
    "        f\"Create an image of a working on a tour plan in a\",\n",
    "        f\"Create an image of a brainstorming new ideas in a\",\n",
    "        f\"Create an image of a actively working on a project in a\",\n",
    "        f\"Create an image of a reflecting on their work in a\",\n",
    "        f\"Create an image of a collaborating with colleagues in a\",\n",
    "        f\"Create an image of a teaching or presenting in a\",\n",
    "        f\"Create an image of a conducting research in a\",\n",
    "        f\"Create an image of a creating an art piece in a\",\n",
    "        f\"Create an image of a solving a complex problem in a\",\n",
    "        f\"Create an image of a giving a speech or a lecture in a\",\n",
    "        f\"Create an image of a experimenting with new techniques in a\"\n",
    "        f\"Create an image of a designing a new invention in a\",\n",
    "        f\"Create an image of a leading a team meeting in a\",\n",
    "        f\"Create an image of a analyzing data on a computer in a\",\n",
    "        f\"Create an image of a writing a book in a\",\n",
    "        f\"Create an image of a gardening in a\",\n",
    "        f\"Create an image of a playing a musical instrument in a\",\n",
    "        f\"Create an image of a practicing yoga in a\",\n",
    "        f\"Create an image of a cooking in a gourmet kitchen in a\",\n",
    "        f\"Create an image of a building a robot in a\",\n",
    "        f\"Create an image of a exploring a historic site in a\"\n",
    "    ]\n",
    "    return prompts[random]\n",
    "\n",
    "descriptions = ['unique', 'distinctive', 'cool']  \n",
    "professions = ['scientist', 'artist', 'professor']  \n",
    "settings = ['corporate office', 'research center', 'classroom'] \n",
    "\n",
    "def create_prompt(a,b,c, prompt):\n",
    "    sentence = prompt[:21] + descriptions[a] + \" \" + professions[b] + \" \" + prompt[21:] + ' ' +settings[c]+ '.'\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ACTION = 3\n",
    "\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StablediffusionEnv(gym.Env):\n",
    "    def __init__(self, pipe):\n",
    "        super(StablediffusionEnv, self).__init__()\n",
    "        self.prompt = None\n",
    "        self.current_word = None\n",
    "        embedding_size = 512\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(embedding_size,), dtype=np.float32)\n",
    "        self.action_space = gym.spaces.Discrete((NUM_ACTION)**3)  \n",
    "        self.prompt_embed = None      \n",
    "        \n",
    "        self.action_dict = {}\n",
    "        count = 0\n",
    "        for i in range(NUM_ACTION):\n",
    "            for j in range(NUM_ACTION):\n",
    "                for k in range(NUM_ACTION):\n",
    "                    self.action_dict[count] = (i, j, k)\n",
    "                    count += 1\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_word = None\n",
    "        self.prompt = get_prompt(random.randint(0, 19))\n",
    "        prompt_clip_text = clip.tokenize([self.prompt]).to(device)  # Ensure the device is consistent\n",
    "        self.prompt_embed = clip_model.encode_text(prompt_clip_text)\n",
    "        return self.prompt_embed.cpu().detach().numpy()  # Convert to numpy after moving to CPU\n",
    "\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        action_count[action] += 1\n",
    "        word1, word2, word3 = self.action_dict[action]\n",
    "        self.current_word = create_prompt(word1,word2,word3,self.prompt)\n",
    "        print(self.current_word)\n",
    "\n",
    "        prediction = pipe([self.current_word]).images\n",
    "        \n",
    "        # plt.imshow(prediction[0])\n",
    "        # plt.axis('off')  \n",
    "        # plt.show()\n",
    "        \n",
    "        #clip embeddings\n",
    "        clip_text = clip.tokenize([self.current_word]).to(device)\n",
    "        \n",
    "        clip_image = clip_preprocess(prediction[0]).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_features = clip_model.encode_image(clip_image)\n",
    "            text_features = clip_model.encode_text(clip_text)\n",
    "\n",
    "        \n",
    "        cosine_similarity = torch.nn.functional.cosine_similarity(text_features, image_features, dim=1)\n",
    "        \n",
    "        reward = 100*(1-cosine_similarity.item())\n",
    "        print('Reward', {reward})\n",
    "        self.last_images = np.array(prediction[0])\n",
    "        done = True\n",
    "        return np.array(prediction[0]).mean(axis=2).mean(axis=1), reward, done, {'image' : prediction[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DummyVecEnv([lambda: StablediffusionEnv(pipe)])\n",
    "dqn_model = DQN(\"MlpPolicy\", env, buffer_size=10000, verbose=1, exploration_final_eps=0.6, exploration_initial_eps=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "action_count = [0] * (NUM_ACTION**3)\n",
    "dqn_model.learn(3000)\n",
    "\n",
    "# # dqn_model.save('dqn_model_lora_SDv4_3000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_model = DQN.load('RL models/dqn_model_lora_SDv4_3000.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mean probabilities\n",
    "\n",
    "proxy_prob_clip = []\n",
    "\n",
    "for episode in range(1000):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    # get Q values\n",
    "    q_values = dqn_model.policy.q_net(torch.tensor(obs, dtype=torch.float32, device = dqn_model.device))\n",
    "    action_probabilities = torch.nn.functional.softmax(q_values, dim=1)\n",
    "    proxy_prob_clip.append(action_probabilities)\n",
    "\n",
    "concatenated_data = torch.cat(proxy_prob_clip, dim=0)\n",
    "transposed_data = concatenated_data.t()\n",
    "\n",
    "proxy_mean = torch.mean(transposed_data, dim=1)\n",
    "proxy_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = []\n",
    "episode_actions = []\n",
    "proxy_prob = []\n",
    "\n",
    "store_img = [[] for _ in range(NUM_ACTION**3)]\n",
    "action_count = [0] * (NUM_ACTION**3)\n",
    "for episode in range(500):\n",
    "    print(f\"Episode: {episode}\")\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    q_values = dqn_model.policy.q_net(torch.tensor(obs, dtype=torch.float32, device = dqn_model.device))\n",
    "    action_probabilities = torch.nn.functional.softmax(q_values, dim=1)\n",
    "    proxy_prob.append(action_probabilities)\n",
    "    while not done:\n",
    "        action, _ = dqn_model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        store_img[action[0]].append(info[0]['image'])\n",
    "        episode_rewards.append(reward)\n",
    "        episode_actions.append(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(episode_rewards)\n",
    "print(episode_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save images\n",
    "\n",
    "list_of_lists_of_images = store_img\n",
    "\n",
    "base_dir = 'saved_images_lora'\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "with open('image_paths.txt', 'w') as file:\n",
    "    for i, sublist in enumerate(list_of_lists_of_images):\n",
    "        sublist_dir = os.path.join(base_dir, f'sublist_{i}')\n",
    "        os.makedirs(sublist_dir, exist_ok=True)\n",
    "\n",
    "        for j, np_image in enumerate(sublist):\n",
    "            # Convert NumPy array to PIL Image\n",
    "            pil_image = Image.fromarray(np.uint8(np_image))\n",
    "            image_path = os.path.join(sublist_dir, f'image_{j}.png')\n",
    "            pil_image.save(image_path)\n",
    "\n",
    "            # Write the path to the file\n",
    "            file.write(image_path + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episode_rewards)\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('reward')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_actions, action_counts = np.unique(np.concatenate(episode_actions), return_counts=True)\n",
    "plt.bar(unique_actions, action_counts, color=\"green\")\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Stable diffusion v1-4')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dict2 = {}\n",
    "count2 = 0\n",
    "for i in range(NUM_ACTION):\n",
    "    for j in range(NUM_ACTION):\n",
    "        for k in range(NUM_ACTION):\n",
    "            action_dict2[count2] = (i, j, k)\n",
    "            count2 += 1\n",
    "action_dict2[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_data = torch.cat(proxy_prob, dim=0)\n",
    "transposed_data = concatenated_data.t()\n",
    "\n",
    "proxy_mean = torch.mean(transposed_data, dim=1)\n",
    "proxy_std = torch.std(transposed_data, dim = 1)\n",
    "\n",
    "proxy_std_log = torch.log(proxy_std)\n",
    "proxy_std_log_np = proxy_std_log.cpu().detach().numpy()\n",
    "sorted_indices = np.argsort(proxy_std_log_np)\n",
    "ranks = np.empty_like(sorted_indices)\n",
    "ranks[sorted_indices] = np.arange(len(proxy_std_log_np))\n",
    "\n",
    "ranks+=1\n",
    "\n",
    "ranks_tensor = torch.tensor(ranks)\n",
    "\n",
    "proxy_mean = proxy_mean.reshape(3,3,3).cpu().detach().numpy()\n",
    "scale = (ranks_tensor.reshape(3,3,3).detach().numpy())*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.array(action_count).mean() - 2*np.array(action_count).std()\n",
    "action_CT = np.array(action_count).reshape(3,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "\n",
    "xs = np.arange(3)\n",
    "ys = np.arange(3)\n",
    "zs = np.arange(3)\n",
    "\n",
    "\n",
    "uncertain_threshold = np.argwhere(action_CT <= threshold)\n",
    "certain_threshold = np.argwhere(action_CT > threshold)\n",
    "\n",
    "# Certain points\n",
    "trace1 = go.Scatter3d(x=xs[certain_threshold[:, 0]],\n",
    "                      y=ys[certain_threshold[:, 1]],\n",
    "                      z=zs[certain_threshold[:, 2]],\n",
    "                      mode='markers',\n",
    "                      name='Certain Area',\n",
    "                      marker=dict(\n",
    "                          size=scale[certain_threshold[:, 0],\n",
    "                                           certain_threshold[:, 1],\n",
    "                                           certain_threshold[:, 2]],  # directly use rank_tensor as size\n",
    "                          symbol='circle',\n",
    "                          color=np.log(proxy_mean[certain_threshold[:, 0],\n",
    "                                                            certain_threshold[:, 1],\n",
    "                                                            certain_threshold[:, 2]]), # apply logarithm on color\n",
    "                          colorbar=dict(thickness=10, ticklen=4, x=1),\n",
    "                          colorscale='Viridis',   # choose a colorscale\n",
    "                          opacity=0.8,\n",
    "                          line=dict(color='Black', width=1)\n",
    "                        ),\n",
    "                      hovertemplate=\"Action 1: %{x}<br>Action 2: %{y}<br>Action 3: %{z}<br>LogSoftQ: %{marker.color}<extra></extra>\",\n",
    "                    )\n",
    "# Uncertain points\n",
    "trace2 = go.Scatter3d(x=xs[uncertain_threshold[:, 0]],\n",
    "                      y=ys[uncertain_threshold[:, 1]],\n",
    "                      z=zs[uncertain_threshold[:, 2]],\n",
    "                      mode='markers',\n",
    "                      name='Uncertain Area',\n",
    "                      marker=dict(\n",
    "                          size=10,\n",
    "                          symbol='square',\n",
    "                          color='rgb(255,0,0)',                # set color to red\n",
    "                          opacity=0.8,\n",
    "                          line=dict(color='Black', width=1)\n",
    "                        ),\n",
    "                      showlegend=True,\n",
    "                      hovertemplate=\"Action 1: %{x}<br>Action 2: %{y}<br>Action 3: %{z}<br><extra></extra>\",\n",
    "                    )\n",
    "layout = go.Layout(height=800, width=800, title='3D Heatmap',\n",
    "                   scene=dict(\n",
    "                              xaxis = dict(title='Action 1'),\n",
    "                              yaxis = dict(title='Action 2'),\n",
    "                              zaxis = dict(title='Action 3')\n",
    "                             ),\n",
    "                  )\n",
    "\n",
    "\n",
    "\n",
    "fig = go.Figure(data=[trace1, trace2], layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distinctive, artist, research center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_prompt(1,0,0,get_prompt(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pipe([create_prompt(1,1,1,get_prompt(random.randint(0, 19)))]).images\n",
    "plt.imshow(prediction[0])\n",
    "plt.axis('off')  \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
