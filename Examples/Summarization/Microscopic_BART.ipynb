{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartModel\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import gym\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "import string\n",
    "from random import randint\n",
    "from pattern.en import pluralize, singularize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This is an example sentence showing off the verb extraction capabilities.\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "verbs = [word for word, tag in tagged if tag.startswith('VB')]\n",
    "adjectives = [word for word, tag in tagged if tag.startswith('JJ')]\n",
    "nouns = [word for word, tag in tagged if tag.startswith('NN')]\n",
    "\n",
    "def check_sentence(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    nouns = [word for word, tag in tagged if tag.startswith('NN')]\n",
    "    verbs = [word for word, tag in tagged if tag.startswith('VB')]\n",
    "    adjectives = [word for word, tag in tagged if tag.startswith('JJ')]\n",
    "\n",
    "    if len(nouns) + len(verbs) +len(adjectives) == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def drop_noun(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    nouns = [word for word, tag in tagged if tag.startswith('NN')]\n",
    "\n",
    "    # If there are nouns, drop a random one\n",
    "    if nouns:\n",
    "        noun_to_drop = random.choice(nouns)\n",
    "        tokens = [token for token in tokens if token != noun_to_drop]\n",
    "        return ' '.join(tokens)\n",
    "    else:\n",
    "        # Return the original sentence if there are no nouns\n",
    "        return sentence\n",
    "\n",
    "def drop_verb(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    verbs = [word for word, tag in tagged if tag.startswith('VB')]\n",
    "\n",
    "    # If there are verbs, drop a random one\n",
    "    if verbs:\n",
    "        verb_to_drop = random.choice(verbs)\n",
    "        tokens = [token for token in tokens if token != verb_to_drop]\n",
    "        return ' '.join(tokens)\n",
    "    else:\n",
    "        # Return the original sentence if there are no verbs\n",
    "        return sentence\n",
    "\n",
    "def drop_adjective(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    adjectives = [word for word, tag in tagged if tag.startswith('JJ')]\n",
    "\n",
    "    # If there are adjectives, drop a random one\n",
    "    if adjectives:\n",
    "        adjective_to_drop = random.choice(adjectives)\n",
    "        tokens = [token for token in tokens if token != adjective_to_drop]\n",
    "        return ' '.join(tokens)\n",
    "    else:\n",
    "        # Return the original sentence if there are no adjectives\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import pipeline, BartTokenizer\n",
    "\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "summarizer = pipeline(\"summarization\", model=model_name)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def get_summary(original_sentence):\n",
    "    # Tokenize the input and truncate if it's too long\n",
    "    tokens = tokenizer.encode(original_sentence, add_special_tokens=False)\n",
    "    max_length_allowed = tokenizer.model_max_length\n",
    "    if len(tokens) > max_length_allowed:\n",
    "        # Truncate the tokens to the maximum length allowed\n",
    "        tokens = tokens[:max_length_allowed]\n",
    "\n",
    "    # Convert tokens back to text\n",
    "    truncated_text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "    # Summarize the truncated text\n",
    "    summary = summarizer(truncated_text, max_length=500, min_length=30, do_sample=False)\n",
    "    return summary[0]['summary_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "\n",
    "def calculate_bleu_score(candidate, reference):\n",
    "    \"\"\"\n",
    "    Calculate the BLEU score for a candidate sentence given a reference sentence.\n",
    "\n",
    "    Args:\n",
    "    candidate (str): The summarized text (candidate translation).\n",
    "    reference (str): The reference text (reference translation).\n",
    "\n",
    "    Returns:\n",
    "    float: The BLEU score.\n",
    "    \"\"\"\n",
    "    bleu = sacrebleu.corpus_bleu([candidate], [[reference]])\n",
    "    return bleu.score\n",
    "\n",
    "candidate_summary = \"The scientist presented her climate change research at a conference, calling for urgent action.\"\n",
    "reference_summary = \"At the conference, the scientist highlighted the need for immediate measures against climate change.\"\n",
    "\n",
    "bleu_score = calculate_bleu_score(candidate_summary, reference_summary)\n",
    "print(f\"BLEU Score: {bleu_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartModel\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import gym\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import torch\n",
    "\n",
    "dataset = load_dataset(\"openai/summarize_from_feedback\", 'axis')\n",
    "\n",
    "action_list = [\n",
    "    drop_noun,\n",
    "    drop_verb,\n",
    "    drop_adjective\n",
    "]\n",
    "\n",
    "action_list_name = [\n",
    "  'drop_noun',\n",
    "    'drop_verb',\n",
    "    'drop_adjective'\n",
    "]\n",
    "\n",
    "sentence = \"Your example sentence here.\"\n",
    "index = 2\n",
    "action_list[index](sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BARTEmbedder:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "        self.model = BartModel.from_pretrained('facebook/bart-large').to(self.device).half()  # Using FP16\n",
    "        self.target_dim = 1024  # Target embedding dimension\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', max_length=1024, truncation=True).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        embeddings = outputs.encoder_last_hidden_state\n",
    "        embeddings = torch.mean(embeddings, dim=1)\n",
    "\n",
    "        # Adjust the embedding to have the target dimension\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        if embedding_dim > self.target_dim:\n",
    "            embeddings = embeddings[:, :self.target_dim]\n",
    "        elif embedding_dim < self.target_dim:\n",
    "            padding = torch.zeros((embeddings.shape[0], self.target_dim - embedding_dim), device=self.device)\n",
    "            embeddings = torch.cat([embeddings, padding], dim=1)\n",
    "\n",
    "        return embeddings.cpu().numpy()\n",
    "\n",
    "\n",
    "class NLPEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(NLPEnv, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.current_word = None\n",
    "        self.ground_truth = None\n",
    "        self.embedder = BARTEmbedder()\n",
    "        embedding_dim = 1024\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(embedding_dim,), dtype=np.float32)\n",
    "        self.action_space = gym.spaces.Discrete(3)\n",
    "        self.count = 0\n",
    "    \n",
    "        action_list = [\n",
    "            drop_noun,\n",
    "            drop_verb,\n",
    "            drop_adjective\n",
    "        ]\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.count = 0\n",
    "        random_index = 0\n",
    "        self.current_word = self.dataset['test']['info'][random_index]['article'][:3500]\n",
    "        self.ground_truth = self.dataset['test']['summary'][random_index]['text'][:3500]\n",
    "        return self.embedder.get_embedding(self.current_word)\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        check = check_sentence(self.current_word)\n",
    "\n",
    "        prediction = action_list[action](self.current_word)\n",
    "        prediction = ' '.join(prediction)\n",
    "        summary = get_summary(prediction)\n",
    "        bleu_score = calculate_bleu_score(prediction, summary)\n",
    "        reward = bleu_score - self.count\n",
    "        done = False\n",
    "        self.current_word = prediction\n",
    "\n",
    "        if check:\n",
    "            done = True\n",
    "        self.count +=1\n",
    "\n",
    "        print(f'Reward: {reward} Action: {action_list_name[action]} Word length: {len(self.current_word)} Prediction length: {len(summary)}')\n",
    "        if reward < 5.0:\n",
    "            done = True\n",
    "            steps.append(self.count) \n",
    "            print('EPISODE COMPLETE')    \n",
    "               \n",
    "        return self.embedder.get_embedding(prediction), reward, done, {'current_word': self.current_word}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DummyVecEnv([lambda: NLPEnv()])\n",
    "dqn_model_bart_multi = DQN(\"MlpPolicy\", env, verbose=1, exploration_final_eps=0.6, exploration_initial_eps=1.0)#DQN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment to learn\n",
    "# steps = []\n",
    "\n",
    "# dqn_model_bart_multi.learn(1000)\n",
    "# # dqn_model_bart_multi.save('dqn_model_bart_multi_steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_model_bart_multi = DQN.load('RL models/dqn_model_bart_multi_steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = []\n",
    "episode_actions = []\n",
    "proxy_prob = []\n",
    "steps = []\n",
    "for episode in range(300):\n",
    "    print(f\"Episode: {episode}\")\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    q_values = dqn_model_bart_multi.policy.q_net(torch.tensor(obs, dtype=torch.float32, device = dqn_model_bart_multi.device))\n",
    "    action_probabilities = torch.nn.functional.softmax(q_values, dim=1)\n",
    "    proxy_prob.append(action_probabilities)\n",
    "    while not done:\n",
    "        action, _ = dqn_model_bart_multi.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        print(f'Actions : {action}, Reward: {reward}')\n",
    "        episode_rewards.append(reward)\n",
    "        episode_actions.append(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of tensors to a single tensor\n",
    "data_tensor = torch.stack(proxy_prob)\n",
    "\n",
    "# Calculate the mean and standard deviation for each index\n",
    "means = torch.mean(data_tensor, dim=0)\n",
    "std_devs = torch.std(data_tensor, dim=0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "means_np = means.cpu().detach().numpy()\n",
    "std_devs_np = std_devs.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "# Data for plotting\n",
    "indices = range(0, 3)\n",
    "\n",
    "# Creating error bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(indices, means_np[0], yerr=std_devs_np[0] * 10, fmt='o', ecolor='red', capsize=5)\n",
    "plt.title('Error Bar Plot of Means and Standard Deviations')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Values')\n",
    "plt.xticks(indices)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
